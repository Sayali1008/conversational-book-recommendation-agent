{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2452d582",
   "metadata": {},
   "source": [
    "# 2026-01-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "915a324e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Shared utilities for the recommendation system pipeline.\n",
    "Provides common functions for data loading, logging, and validation.\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "from pydantic import ConfigDict, validate_call\n",
    "\n",
    "\n",
    "def setup_logging(stage_name: str, log_file: str, level=logging.INFO):\n",
    "    \"\"\"Configure logging for a pipeline stage.\"\"\"\n",
    "    os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=level,\n",
    "        format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\",\n",
    "        filename=log_file,\n",
    "        filemode=\"w\",\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "    logger = logging.getLogger(stage_name)\n",
    "\n",
    "    # Prevent duplicate handlers if main() is called multiple times\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "\n",
    "    # File Handler\n",
    "    file_handler = logging.FileHandler(log_file, mode=\"a\", encoding=\"utf-8\")\n",
    "    file_handler.setFormatter(\n",
    "        logging.Formatter(\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\")\n",
    "    )\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    # Console Handler\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setFormatter(\n",
    "        logging.Formatter(\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "    )\n",
    "\n",
    "    # Explicitly ensure the stream is flushed after every write\n",
    "    console_handler.flush = sys.stdout.flush\n",
    "    logger.addHandler(console_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "\n",
    "@validate_call(config=ConfigDict(arbitrary_types_allowed=True))\n",
    "def safe_read_csv(filepath: str, usecols: Optional[list[str]] = None) -> pd.DataFrame:\n",
    "    \"\"\"Safely read CSV file\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"File not found: {filepath}\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(filepath).astype(str).fillna(\"\")\n",
    "\n",
    "        df.columns = df.columns.str.lower()\n",
    "        if usecols:\n",
    "            missing_cols = [c for c in usecols if c.lower() not in df.columns]\n",
    "            if missing_cols:\n",
    "                raise ValueError(f\"Missing columns in input CSV: {missing_cols}\")\n",
    "            return df[usecols]\n",
    "        return df\n",
    "    except pd.errors.ParserError as e:\n",
    "        raise pd.errors.ParserError(f\"Error parsing {filepath}: {e}\")\n",
    "\n",
    "\n",
    "@validate_call(config=ConfigDict(arbitrary_types_allowed=True))\n",
    "def safe_read_feather(\n",
    "    filepath: str, usecols: Optional[list[str]] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Safely read Feather file\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"File not found: {filepath}\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_feather(filepath).astype(str).fillna(\"\")\n",
    "\n",
    "        df.columns = df.columns.str.lower()\n",
    "        if usecols:\n",
    "            missing_cols = [c for c in usecols if c.lower() not in df.columns]\n",
    "            if missing_cols:\n",
    "                raise ValueError(f\"Missing columns in input Feather: {missing_cols}\")\n",
    "            return df[usecols]\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error reading or processing feather file {filepath}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "269d2d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Centralized configuration for the recommendation system pipeline.\n",
    "Defines all paths, hyperparameters, and constants used across stages.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================\n",
    "# Project Structure\n",
    "# ============================================================\n",
    "PROJECT_ROOT = Path(\"/Users/sayalimoghe/Documents/Career/GitHub/conversational-book-recommendation-agent\")\n",
    "\n",
    "# Data paths\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "RAW_DATA_DIR = DATA_DIR / \"raw\"\n",
    "CLEAN_DATA_DIR = DATA_DIR / \"clean\"\n",
    "EMBEDDINGS_DIR = DATA_DIR / \"embeddings\"\n",
    "MATRICES_DIR = DATA_DIR / \"matrices\"\n",
    "PKL_DIR = DATA_DIR / \"pkl\"\n",
    "FACTORS_DIR = DATA_DIR / \"factors\"\n",
    "LOGS_DIR = PROJECT_ROOT / \"logs\"\n",
    "LOG_FILE = str(LOGS_DIR / \"app.log\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in [\n",
    "    CLEAN_DATA_DIR,\n",
    "    EMBEDDINGS_DIR,\n",
    "    MATRICES_DIR,\n",
    "    PKL_DIR,\n",
    "    FACTORS_DIR,\n",
    "    LOGS_DIR,\n",
    "]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# ============================================================\n",
    "# Stage 1: Data Preprocessing\n",
    "# ============================================================\n",
    "# Input files\n",
    "INPUT_BOOKS = str(RAW_DATA_DIR / \"books_data.csv\")\n",
    "INPUT_RATINGS = str(RAW_DATA_DIR / \"books_rating.csv\")\n",
    "INPUT_COLS_BOOKS = [\"title\", \"description\", \"authors\", \"infolink\", \"categories\"]\n",
    "INPUT_COLS_RATINGS = [\n",
    "    \"title\",\n",
    "    \"user_id\",\n",
    "    \"profilename\",\n",
    "    \"review/helpfulness\",\n",
    "    \"review/score\",\n",
    "    \"review/time\",\n",
    "    \"review/summary\",\n",
    "    \"review/text\",\n",
    "]\n",
    "\n",
    "# Output files\n",
    "OUTPUT_BOOKS = str(CLEAN_DATA_DIR / \"cleaned_books_data.ftr\")\n",
    "OUTPUT_COLS_BOOKS = [\"book_id\", \"title\", \"authors\", \"description\", \"genres\", \"infolink\"]\n",
    "OUTPUT_RATINGS = str(CLEAN_DATA_DIR / \"cleaned_ratings_data.ftr\")\n",
    "OUTPUT_COLS_RATINGS = [\n",
    "    \"book_id\",\n",
    "    \"user_id\",\n",
    "    \"review/score\",\n",
    "    \"confidence\",\n",
    "    \"datetime\",\n",
    "    \"review/summary\",\n",
    "    \"review/text\",\n",
    "]\n",
    "\n",
    "# Configuration\n",
    "MIN_DESC_LENGTH = 10\n",
    "TOP_N_GENRES = 50\n",
    "COMMON_DELIMS = [\";\", \"|\", \"/\", \"•\"]\n",
    "MIN_USER_INTERACTIONS = 5\n",
    "MAX_USER_INTERACTIONS = 500\n",
    "MIN_BOOK_INTERACTIONS = 5\n",
    "\n",
    "# ============================================================\n",
    "# Stage 2: Semantic Search (Embeddings)\n",
    "# ============================================================\n",
    "# Output files\n",
    "OUTPUT_CATALOG_BOOKS_INDEX = str(EMBEDDINGS_DIR / \"catalog_books.index\")\n",
    "OUTPUT_CATALOG_BOOKS_EMBEDDINGS = str(EMBEDDINGS_DIR / \"catalog_books.npy\")\n",
    "\n",
    "# Configuration\n",
    "BATCH_SIZE = 64\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"  # or \"all-mpnet-base-v2\"\n",
    "# Example output: (num_rows, 384) for all-MiniLM-L6-v2\n",
    "# Example output: (num_rows, 768) for all-mpnet-base-v2\n",
    "\n",
    "# ============================================================\n",
    "# Stage 3: Build Interaction Matrix\n",
    "# ============================================================\n",
    "# Input files\n",
    "USER_IDX_PKL = str(PKL_DIR / \"user_to_idx.pkl\")\n",
    "BOOK_IDX_PKL = str(PKL_DIR / \"book_to_idx.pkl\")\n",
    "\n",
    "# Output files\n",
    "OUTPUT_TRAIN_MATRIX = str(MATRICES_DIR / \"train_matrix.npz\")\n",
    "OUTPUT_VAL_MATRIX = str(MATRICES_DIR / \"val_matrix.npz\")\n",
    "OUTPUT_TEST_MATRIX = str(MATRICES_DIR / \"test_matrix.npz\")\n",
    "\n",
    "# Configuration\n",
    "TRAIN_TEST_SPLIT = 0.8\n",
    "VAL_TEST_SPLIT = 0.5\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# ============================================================\n",
    "# Stage 4: Train Collaborative Filtering\n",
    "# ============================================================\n",
    "# Output files\n",
    "OUTPUT_ALS_MODEL = str(PKL_DIR / \"als_model.pkl\")\n",
    "OUTPUT_USER_FACTORS = str(FACTORS_DIR / \"user_factors.npy\")\n",
    "OUTPUT_BOOK_FACTORS = str(FACTORS_DIR / \"book_factors.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c593afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import html\n",
    "\n",
    "import pandas as pd\n",
    "from pydantic import ConfigDict, validate_call\n",
    "\n",
    "# from config import *\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Helper methods\n",
    "# ----------------------\n",
    "def parse_list_value(val):\n",
    "    \"\"\"\n",
    "    Parse a string value into a list of cleaned strings.\n",
    "    Args: val (str or NaN): The input string representing a list or comma-separated values.\n",
    "    Returns: list[str]: A list of stripped strings, empty if input is NaN, empty, or cannot be parsed.\n",
    "    \"\"\"\n",
    "    if pd.isna(val) or val.strip() == \"\":\n",
    "        return []\n",
    "    val = val.strip()\n",
    "\n",
    "    # Try to parse as Python list\n",
    "    if val.startswith(\"[\") and val.endswith(\"]\"):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(val)\n",
    "            return [c.strip() for c in parsed if isinstance(c, str)]\n",
    "        except Exception:\n",
    "            return []\n",
    "\n",
    "    # Otherwise, comma-separated\n",
    "    for sep in COMMON_DELIMS:\n",
    "        val = val.replace(sep, \",\")\n",
    "\n",
    "    return [c.strip() for c in val.split(\",\") if c.strip()]\n",
    "\n",
    "\n",
    "def normalize_author_column(author_col):\n",
    "    \"\"\"\n",
    "    Normalize and clean a DataFrame column containing author names.\n",
    "    Args: author_col (pd.Series): Column of raw author strings.\n",
    "    Returns: pd.Series: Column where each row is a sorted list of valid author names in title case.\n",
    "    \"\"\"\n",
    "    # basic cleanup before parsing\n",
    "    cleaned_col = (\n",
    "        author_col.fillna(\"\")\n",
    "        .astype(str)\n",
    "        .str.replace(\";\", \",\", regex=False)\n",
    "        .str.replace(\"&\", \",\", regex=False)\n",
    "        .str.replace(r\"\\s+and\\s+\", \",\", regex=True)\n",
    "        .str.replace(r\"\\(.*?editor.*?\\)\", \"\", regex=True, case=False)\n",
    "        .str.replace(r\"\\beditor\\b\", \"\", regex=True, case=False)\n",
    "        .str.replace(r\"\\bed\\.\\b\", \"\", regex=True, case=False)\n",
    "        .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "        .str.strip()\n",
    "    )\n",
    "\n",
    "    # parse each row value in the column\n",
    "    cleaned_col = cleaned_col.apply(parse_list_value)\n",
    "\n",
    "    # convert valid authors to title case and remove invalid ones\n",
    "    invalid_authors = {\"unknown\"}\n",
    "    cleaned_col = cleaned_col.apply(\n",
    "        lambda author_row: sorted(\n",
    "            {item.title() for item in author_row if item.lower() not in invalid_authors}\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return cleaned_col\n",
    "\n",
    "\n",
    "def normalize_genre_column(genre_col):\n",
    "    \"\"\"\n",
    "    Normalize and clean a column of genres, converting to lowercase, trimming, removing short/invalid entries, and sorting.\n",
    "    Args: genre_col (pd.Series): Column of genre strings.\n",
    "    Returns: pd.Series: Column where each row is a sorted list of valid genres.\n",
    "    \"\"\"\n",
    "    cleaned_col = (\n",
    "        genre_col.fillna(\"\")\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .str.replace(\"&\", \"and\", regex=False)\n",
    "    )\n",
    "\n",
    "    # parse each row value in the column\n",
    "    cleaned_col = cleaned_col.apply(parse_list_value)\n",
    "\n",
    "    # lowercase, trim, remove very short/invalid entries\n",
    "    cleaned_col = cleaned_col.apply(\n",
    "        lambda lst: [g.lower().strip() for g in lst if len(g) > 2 and \"...\" not in g]\n",
    "    )\n",
    "\n",
    "    # sort\n",
    "    cleaned_col = cleaned_col.apply(lambda lst: sorted(set(lst)))\n",
    "\n",
    "    return cleaned_col\n",
    "\n",
    "\n",
    "def map_genres_to_top_or_other(genres_row, top_genres):\n",
    "    \"\"\"\n",
    "    Map genres to a top-N list, replacing non-top genres with 'other'.\n",
    "    Args:\n",
    "        genres_row (list[str]): List of genres for a book.\n",
    "        top_genres (set[str]): Set of top-N genres.\n",
    "    Returns: list[str]: Sorted list with genres mapped to top or 'other'.\n",
    "    \"\"\"\n",
    "    # any genre not in top_genres → \"other\"\n",
    "    genre_list = set()\n",
    "    for g in genres_row:\n",
    "        if g in top_genres:\n",
    "            genre_list.add(g)\n",
    "        else:\n",
    "            genre_list.add(\"other\")\n",
    "\n",
    "    return sorted(genre_list)\n",
    "\n",
    "\n",
    "def reduce_to_top_genres(genre_col):\n",
    "    \"\"\"\n",
    "    Reduce genres in a column to top-N most common genres, mapping all other genres to 'other'.\n",
    "    Args: genre_col (pd.Series): Column of lists of genres.\n",
    "    Returns: pd.Series: Column of lists with top-N genres or 'other'.\n",
    "    \"\"\"\n",
    "    all_genres = genre_col.explode()\n",
    "    top_genres = set(all_genres.value_counts().head(TOP_N_GENRES).index)\n",
    "    return genre_col.apply(map_genres_to_top_or_other, args=(top_genres,))\n",
    "\n",
    "\n",
    "def normalize_text_field(cleaned_col):\n",
    "    \"\"\"\n",
    "    Clean a text column by removing HTML tags, unescaping HTML entities, removing escaped characters and control characters, and collapsing whitespace.\n",
    "    Args: cleaned_col (pd.Series): Column of strings.\n",
    "    Returns: pd.Series: Cleaned string column.\n",
    "    \"\"\"\n",
    "    cleaned_col = cleaned_col.fillna(\"\").astype(str)\n",
    "    cleaned_col = cleaned_col.str.replace(r\"<[^>]+>\", \"\", regex=True)\n",
    "    cleaned_col = cleaned_col.apply(html.unescape)\n",
    "    cleaned_col = cleaned_col.str.replace(r\"[\\n\\t\\r]\", \" \", regex=True)\n",
    "    cleaned_col = cleaned_col.str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    cleaned_col = cleaned_col.apply(\n",
    "        lambda s: \"\".join(ch for ch in s if ch.isprintable())\n",
    "    )\n",
    "\n",
    "    return cleaned_col\n",
    "\n",
    "\n",
    "def keep_usable_books(df):\n",
    "    \"\"\"\n",
    "    Filter books to retain only those with complete and valid metadata: title, at least one author, at least one genre, and description length >= MIN_DESC_LENGTH.\n",
    "    Args: df (pd.DataFrame): DataFrame containing book metadata.\n",
    "    Returns: tuple: (filtered DataFrame of usable books, fraction of books that are usable)\n",
    "    \"\"\"\n",
    "    mask = (\n",
    "        (df[\"title\"].notna() & df[\"title\"].str.strip() != \"\")\n",
    "        & (df[\"description\"].str.strip().str.len() >= MIN_DESC_LENGTH)\n",
    "        & (df[\"authors\"].notna() & df[\"authors\"].apply(lambda lst: len(lst) > 0))\n",
    "        & (df[\"genres\"].apply(lambda lst: len(lst) > 0))\n",
    "    )\n",
    "\n",
    "    return df[mask].copy(), mask.mean()\n",
    "\n",
    "\n",
    "@validate_call(config=ConfigDict(arbitrary_types_allowed=True))\n",
    "def clean_books_data(logger, books_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean and normalize the books DataFrame including authors, genres, titles, and descriptions.\n",
    "    Args: books_df (pd.DataFrame): Raw books DataFrame.\n",
    "    Returns: pd.DataFrame: Cleaned catalog of usable books.\n",
    "    \"\"\"\n",
    "    books_df[\"authors\"] = normalize_author_column(books_df[\"authors\"])\n",
    "    books_df[\"genres\"] = reduce_to_top_genres(\n",
    "        normalize_genre_column(books_df[\"categories\"])\n",
    "    )\n",
    "    books_df[\"title\"] = normalize_text_field(books_df[\"title\"])\n",
    "    books_df[\"description\"] = normalize_text_field(books_df[\"description\"])\n",
    "\n",
    "    # A usable book must have: title, at least one author, description length ≥ X, at least one category\n",
    "    books_df, usable_ratio = keep_usable_books(books_df)\n",
    "    logger.info(f\"Usable books: {usable_ratio:.2%}\")\n",
    "\n",
    "    # NOTE: No title + author duplicates were found in this dataset\n",
    "\n",
    "    # Create a unique sequential integer starting from 0 for each book\n",
    "    books_df[\"book_id\"] = range(1, len(books_df) + 1)\n",
    "\n",
    "    return books_df\n",
    "\n",
    "\n",
    "@validate_call(config=ConfigDict(arbitrary_types_allowed=True))\n",
    "def clean_ratings_data(logger, ratings_df, cleaned_books_df) -> pd.DataFrame:\n",
    "    \"\"\"Clean ratings DataFrame, normalize titles, convert timestamps, deduplicate, transform scores to confidence, and filter by user/item thresholds.\"\"\"\n",
    "\n",
    "    # Drop rows with missing title, user_id, review/score\n",
    "    ratings_df = ratings_df[ratings_df[\"title\"].notna()]\n",
    "    ratings_df = ratings_df[ratings_df[\"user_id\"].notna()]\n",
    "    ratings_df = ratings_df[ratings_df[\"review/score\"].notna()]\n",
    "\n",
    "    # Normalize titles with same techniques as for books data\n",
    "    ratings_df[\"title\"] = normalize_text_field(ratings_df[\"title\"])\n",
    "    ratings_df = pd.merge(\n",
    "        ratings_df, cleaned_books_df[[\"book_id\", \"title\"]], on=\"title\", how=\"inner\"\n",
    "    )\n",
    "    logger.info(f\"ratings_df.columns: {ratings_df.columns.tolist()}\")\n",
    "\n",
    "    # Parse review/time from epoch time to datetime UTC\n",
    "    ratings_df[\"datetime\"] = pd.to_datetime(\n",
    "        pd.to_numeric(ratings_df[\"review/time\"]), unit=\"s\", utc=True\n",
    "    )\n",
    "\n",
    "    # Deduplicate (user, book) pairs, group by (user_id, title) and keep one with most recent review/time\n",
    "    ratings_df = ratings_df.sort_values(\"review/time\").drop_duplicates(\n",
    "        subset=[\"user_id\", \"title\"], keep=\"last\"\n",
    "    )\n",
    "\n",
    "    # Transform 1-5 ratings into confidence weights: scores ≤3 become 0, 4 becomes 1, and 5 becomes 2\n",
    "    ratings_df[\"review/score\"] = pd.to_numeric(\n",
    "        ratings_df[\"review/score\"], errors=\"coerce\"\n",
    "    )\n",
    "    ratings_df[\"confidence\"] = ratings_df[\"review/score\"].clip(lower=3) - 3\n",
    "\n",
    "    # Filter out interactions with zero confidence (ratings ≤ 3)\n",
    "    logger.info(f\"Ratings data size before filtering confidence=0: {len(ratings_df):,}\")\n",
    "    len_confidence_0 = len(ratings_df[ratings_df[\"confidence\"] == 0])\n",
    "    ratings_df = ratings_df[ratings_df[\"confidence\"] > 0].copy()\n",
    "\n",
    "    logger.info(f\"Ratings data size after filtering confidence=0: {len(ratings_df):,}\")\n",
    "    logger.info(f\"Removed {len_confidence_0:,} zero-confidence rows\")\n",
    "    return ratings_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56d84bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-05 19:46:53,470 [INFO] Usable books: 67.71%\n",
      "2026-01-05 19:47:36,744 [INFO] ratings_df.columns: ['title', 'user_id', 'profilename', 'review/helpfulness', 'review/score', 'review/time', 'review/summary', 'review/text', 'book_id']\n",
      "2026-01-05 19:47:41,207 [INFO] Ratings data size before filtering confidence=0: 1,705,374\n",
      "2026-01-05 19:47:41,822 [INFO] Ratings data size after filtering confidence=0: 1,360,308\n",
      "2026-01-05 19:47:41,822 [INFO] Removed 345,066 zero-confidence rows\n",
      "2026-01-05 19:47:42,097 [INFO] ✓ Ratings data cleaned: shape=(1360308, 11)\n"
     ]
    }
   ],
   "source": [
    "logger = setup_logging(\"analysis\", \"logs/analysis.log\")\n",
    "\n",
    "books_df = safe_read_csv(INPUT_BOOKS, INPUT_COLS_BOOKS)\n",
    "catalog_books_df = clean_books_data(logger, books_df)\n",
    "\n",
    "ratings_df = safe_read_csv(INPUT_RATINGS, INPUT_COLS_RATINGS)\n",
    "ratings_df = clean_ratings_data(logger, ratings_df, catalog_books_df)\n",
    "logger.info(f\"✓ Ratings data cleaned: shape={ratings_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5fff863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-09 23:51:19,228 [INFO] Users: 25,999\n",
      "2026-01-09 23:51:19,233 [INFO] CF-trainable books: 12,183\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    prev_len = len(ratings_df)\n",
    "\n",
    "    user_counts = ratings_df[\"user_id\"].value_counts()\n",
    "    ratings_df = ratings_df[\n",
    "        ratings_df[\"user_id\"].isin(\n",
    "            user_counts[\n",
    "                (user_counts >= MIN_USER_INTERACTIONS)\n",
    "                & (user_counts <= MAX_USER_INTERACTIONS)\n",
    "            ].index\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    book_counts = ratings_df[\"book_id\"].value_counts()\n",
    "    ratings_df = ratings_df[\n",
    "        ratings_df[\"book_id\"].isin(\n",
    "            book_counts[book_counts >= MIN_BOOK_INTERACTIONS].index\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    if len(ratings_df) == prev_len:\n",
    "        break\n",
    "\n",
    "\n",
    "# Create Index Mappings\n",
    "unique_users = ratings_df[\"user_id\"].unique()\n",
    "unique_books = ratings_df[\"book_id\"].unique()\n",
    "# Example: unique_books = [5, 12, 8, 100, 7, ...]  (in order of appearance)\n",
    "\n",
    "n_users = len(unique_users)\n",
    "n_cf_books = len(unique_books)\n",
    "\n",
    "logger.info(\"Users: %s\", f\"{n_users:,}\")\n",
    "logger.info(\"CF-trainable books: %s\", f\"{n_cf_books:,}\")\n",
    "# 2026-01-05 19:44:45,116 [INFO] Users: 25,999\n",
    "# 2026-01-05 19:44:45,117 [INFO] CF-trainable books: 12,183"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "718f2687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_index_mappings(pkl_file):\n",
    "    \"\"\"\n",
    "    Load item index mappings from pickle files.\n",
    "\n",
    "    Returns:\n",
    "        item_to_idx: dict mapping item_id (book or user) → CF matrix column index\n",
    "        idx_to_item_id: dict mapping CF matrix column index → item_id\n",
    "    \"\"\"\n",
    "    with open(pkl_file, \"rb\") as f:\n",
    "        item_to_idx = pickle.load(f)\n",
    "\n",
    "    # Create reverse mapping: CF index → item_id\n",
    "    idx_to_item_id = {cf_idx: item_id for item_id, cf_idx in item_to_idx.items()}\n",
    "\n",
    "    return item_to_idx, idx_to_item_id\n",
    "\n",
    "def build_cf_to_catalog_mapping(idx_to_book_id):\n",
    "    \"\"\"\n",
    "    Build mapping from CF book indices to catalog indices.\n",
    "    \n",
    "    Args:\n",
    "        idx_to_book_id: Mapping from CF index to book_id\n",
    "    \n",
    "    Returns:\n",
    "        dict: CF index → catalog index mapping\n",
    "    \"\"\"\n",
    "    cf_to_catalog_map = {}\n",
    "    for cf_idx, book_id in idx_to_book_id.items():\n",
    "        catalog_idx = book_id - 1  # book_id starts at 1, catalog indices start at 0\n",
    "        cf_to_catalog_map[cf_idx] = catalog_idx\n",
    "    \n",
    "    return cf_to_catalog_map\n",
    "\n",
    "user_to_idx, idx_to_user_id = load_index_mappings(USER_IDX_PKL)\n",
    "book_to_idx, idx_to_book_id = load_index_mappings(BOOK_IDX_PKL)\n",
    "cf_to_catalog_map = build_cf_to_catalog_mapping(idx_to_book_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ed99a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                                   Dr. Seuss: American Icon\n",
       "description    Philip Nel takes a fascinating look into the k...\n",
       "authors                                             [Philip Nel]\n",
       "infolink       http://books.google.nl/books?id=IjvHQsCn_pgC&d...\n",
       "categories                         ['Biography & Autobiography']\n",
       "genres                             [biography and autobiography]\n",
       "book_id                                                        1\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalog_books_df.head()\n",
    "catalog_books_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c626e42d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                                   Dr. Seuss: American Icon\n",
       "description    Philip Nel takes a fascinating look into the k...\n",
       "authors                                             [Philip Nel]\n",
       "infolink       http://books.google.nl/books?id=IjvHQsCn_pgC&d...\n",
       "categories                         ['Biography & Autobiography']\n",
       "genres                             [biography and autobiography]\n",
       "book_id                                                        1\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalog_books_df[catalog_books_df[\"book_id\"] == 47723]\n",
    "catalog_books_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574bed4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12183\n",
      "12183\n"
     ]
    }
   ],
   "source": [
    "print(len(book_to_idx)) # n_cf_books\n",
    "print(len(cf_to_catalog_map)) # n_cf_books\n",
    "\n",
    "print(cf_to_catalog_map[199])\n",
    "print(idx_to_book_id[199])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ba03ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id\n",
       "A1G37DFO8MQW0M    479\n",
       "A1EKTLUL24HDG8    381\n",
       "A1T17LMQABMBN5    314\n",
       "A1NC9AGZOBI0M1    306\n",
       "AHXAPVSHPJ6OJ     304\n",
       "A1MC6BFHWY6WC3    296\n",
       "A2GBJQ9THOYDAJ    290\n",
       "A2ODBHT4URXVXQ    289\n",
       "A319KYEIAZ3SON    289\n",
       "A30KEXFT9SILL6    287\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10_users = (\n",
    "    ratings_df\n",
    "    .groupby('user_id')\n",
    "    .size()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(10)\n",
    ")\n",
    "top_10_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b467e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5647, 238, 1242, 7158, 2, 3897, 594, 3309, 131, 145]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_indices = [user_to_idx[user_id] for user_id in top_10_users.index]\n",
    "user_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3c96897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id\n",
       "A24MDW8RHNBRON    5\n",
       "A5JWW7BDKKQWW     5\n",
       "A5JWI3NM9N97D     5\n",
       "A29PQDCZSDEULN    5\n",
       "A29QQSMOLH34P6    5\n",
       "A29QZZUCOZQSAS    5\n",
       "A5JIJQ50U3829     5\n",
       "A29ROQMSJIXZ2M    5\n",
       "A29OFQ7ZDJLCI2    5\n",
       "A29S5DRKSYUIHU    5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottom_10_users = (\n",
    "    ratings_df\n",
    "    .groupby('user_id')\n",
    "    .size()\n",
    "    .sort_values(ascending=True)\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "bottom_10_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2af4c594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9393, 21756, 4570, 8260, 25932, 3818, 17023, 20445, 118, 18286]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottom_user_indices = [user_to_idx[user_id] for user_id in bottom_10_users.index]\n",
    "bottom_user_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c484664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (143815, 768)\n",
      "Number of dimensions: 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the .npy file\n",
    "data = np.load('/Users/sayalimoghe/Documents/Career/GitHub/conversational-book-recommendation-agent/data/embeddings/catalog_books_384.npy')\n",
    "\n",
    "# Get the dimensions\n",
    "shape = data.shape  # Returns a tuple (e.g., (100, 50) for 100 rows and 50 columns)\n",
    "num_dims = data.ndim  # Returns an integer (e.g., 2)\n",
    "\n",
    "print(f\"Shape: {shape}\")\n",
    "print(f\"Number of dimensions: {num_dims}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d371d9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension (d): 768\n",
      "Total Vectors (ntotal): 143815\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "# Load the .index file\n",
    "index = faiss.read_index('/Users/sayalimoghe/Documents/Career/GitHub/conversational-book-recommendation-agent/data/embeddings/catalog_books_768.index')\n",
    "\n",
    "# Get key properties\n",
    "dimension = index.d        # The size of each vector (e.g., 768)\n",
    "total_vectors = index.ntotal # The number of vectors currently in the index\n",
    "\n",
    "print(f\"Dimension (d): {dimension}\")\n",
    "print(f\"Total Vectors (ntotal): {total_vectors}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2918b80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ratings_df = pd.read_feather('/Users/sayalimoghe/Documents/Career/GitHub/conversational-book-recommendation-agent/data/clean/cleaned_ratings_data.ftr')\n",
    "books_df = pd.read_feather('/Users/sayalimoghe/Documents/Career/GitHub/conversational-book-recommendation-agent/data/clean/cleaned_books_data.ftr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3092f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      user_id  review/score\n",
      "701770                    nan         45158\n",
      "24354          A14OJS0VWMOSWO          4248\n",
      "597650            AFVQZQ8PW0L          2775\n",
      "605360          AHD101501WCN1          1060\n",
      "103317         A1K1JW1C5CUSUZ           780\n",
      "...                       ...           ...\n",
      "271734         A2GR3JJ113IPNO             1\n",
      "271736         A2GR3SJIXGILE5             1\n",
      "271737         A2GR4AYN9175CS             1\n",
      "271738         A2GR4MU0DZ133O             1\n",
      "0       A00109803PZJ91RLT7DPN             1\n",
      "\n",
      "[701771 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "ratings_df = ratings_df[ratings_df['review/score'] > 3]\n",
    "top_users = (ratings_df.groupby('user_id').size().reset_index(name='review/score').sort_values(by='review/score', ascending=False))\n",
    "print(top_users)\n",
    "# A14OJS0VWMOSWO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aec0e0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file_path = '/Users/sayalimoghe/Documents/Career/GitHub/conversational-book-recommendation-agent/data/pkl/user_to_idx.pkl'\n",
    "with open(file_path, 'rb') as f:\n",
    "    user_pkl = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34ace36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A14OJS0VWMOSWO 16\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "for k, v in user_pkl.items():\n",
    "    if k == 'A14OJS0VWMOSWO':\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbbc6f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A8DJ9EU2QP2JM'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_map = {val: key for key, val in user_pkl.items()}\n",
    "idx_map[100]\n",
    "\n",
    "# unique key to cf idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9a084e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ee928e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book_agent_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
